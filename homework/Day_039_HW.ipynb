{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [作業重點]\n",
    "清楚了解 L1, L2 的意義與差異為何，並了解 LASSO 與 Ridge 之間的差異與使用情境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請閱讀相關文獻，並回答下列問題\n",
    "\n",
    "[脊回歸 (Ridge Regression)](https://blog.csdn.net/daunxx/article/details/51578787)\n",
    "[Linear, Ridge, Lasso Regression 本質區別](https://www.zhihu.com/question/38121173)\n",
    "\n",
    "1. LASSO 回歸可以被用來作為 Feature selection 的工具，請了解 LASSO 模型為什麼可用來作 Feature selection\n",
    "2. 當自變數 (X) 存在高度共線性時，Ridge Regression 可以處理這樣的問題嗎?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.LASSO 回歸可以被用來作為 Feature selection 的工具，請了解 LASSO 模型為什麼可用來作 Feature selection\n",
    "Ans. Lasso模型不僅能使用正規化(regulariztion)來優化模型，亦可以自動執行變數篩選(Feature selection)。\n",
    "在log(λ)=−6時，所有8個變數（圖表上方數字）都包還在模型內，而當在log(λ)=−3時只剩下6個變數，\n",
    "最後當在log(λ)=−1時，只剩2個變數被保留在模型內。\n",
    "因此，當遇到資料變數非常多時，Lasso模型是可以幫你識別並挑選出有最強（也最一致）訊號的變數。\n",
    "\n",
    "Respond Ans. 當模型的目標函數加入 L1 正則化後，會傾向讓用不到的特徵的權重降為 ０，因此實務上可以替我們做到\n",
    "feature selection 的功用。詳細的數學原理可以參考台大資工林軒田教授的影片推導"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.當自變數 (X) 存在高度共線性時，Ridge Regression 可以處理這樣的問題嗎?\n",
    "Ans.通過放棄最小二乘法的無偏性，以損失部分資訊、降低精度為代價來獲得更實際和可靠性更強的迴歸係數。\n",
    "\n",
    "Respond Ans.\\2. 可以。Ridge Regeression 就是透過 L2 進行權重的 decay，進而減少特徵的共線性帶來的 Over-fitting 情形。後面的課程會提到一種非監督室的方法叫做 PCA, Principal Component Analysis 主成分分析，是一種用來消除特徵具有共線性的方法，\n",
    "Ridge Regression 有些地方其實與 PCA 非常類似，建議同學可以先預習這些文章。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS(Ordinary Least Squares)最小平方和線性回歸的最佳化目標函式就是尋找一個平面，使得預測與實際值的誤差平方和(Sum of Squared Error, SSE)最小化\n",
    "\n",
    "當特徵變數量增加(p增加)，我們常會遇到的三個主要問題包括：\n",
    "\n",
    "1. Multicollinearity 多元共線性: 當模型存在共線性時，回歸係數項就會變得非常不穩定(high variance, 高變異)。\n",
    "\n",
    "2. Insufficient Solution 解決方案不充分\n",
    "當特徵個數(p)超過觀測個數(n)(p>n)時，OLS(最小平方法)回歸解矩陣是不可逆的(solution matrix is not invertible)。這代表(1)最小平方估計參數解不是唯一。會存在無限的可用的解，但這些解大多都過度配適資料。(2)在大多數的情況下，這些結果在計算上是不可行的(computationally infeasible)。\n",
    "\n",
    "因此，只能透過移除特徵變數直到(n>p)再將資料投入最小平方回歸模型進行配適。雖然可以透過人工的方式事前處理特徵變數過多的問題，但可能很麻煩且容易出錯。\n",
    "\n",
    "3. Interpretability 可解釋性\n",
    "當我們的特徵變數個數量很大時，我們會希望識別出具有最強解釋效果的較小子集合(Subsetting)。通常我們會偏好透過變數選取(feature selection)的方法來解決。其中一個變數選取法叫做「hard thresholding feature selection(硬閾值特徵選取)」，可以透過線性模型選取(linear model selection)來進行（best subsets & stepwise regression)，但這個方法通常計算上效率較低也不好擴展，而且是直接透過增加或減少特徵變數的方式來進行模型比較。另一個方法叫做「soft thresholding feature selection(軟閾值特徵選取)」，此法將慢慢的將特徵效果推向0。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
